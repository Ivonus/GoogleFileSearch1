from flask import Flask, render_template, request, jsonify, Response, stream_with_context
from flask_cors import CORS
import requests
import os
from dotenv import load_dotenv
import logging
from werkzeug.utils import secure_filename
import mimetypes
import time
import tempfile
import json
from datetime import datetime, timedelta
from typing import Dict, Optional

# Carica variabili d'ambiente
load_dotenv()

# Configurazione logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

app = Flask(__name__, 
            template_folder='../frontend/templates',
            static_folder='../frontend/static')
CORS(app)

# Configurazione
GEMINI_API_KEY = os.getenv('GEMINI_API_KEY')
FILE_SEARCH_STORE_NAME = os.getenv('FILE_SEARCH_STORE_NAME')
DEFAULT_MODEL = os.getenv('DEFAULT_MODEL', 'gemini-2.5-pro')
DEFAULT_CHUNK_SIZE = int(os.getenv('CHUNK_SIZE', '512'))
CHUNK_OVERLAP_PERCENT = int(os.getenv('CHUNK_OVERLAP_PERCENT', '10'))
RESULTS_COUNT = int(os.getenv('RESULTS_COUNT', '25'))
MIN_RELEVANCE_SCORE = float(os.getenv('MIN_RELEVANCE_SCORE', '0.3'))
MAX_CHUNKS_FOR_GENERATION = int(os.getenv('MAX_CHUNKS_FOR_GENERATION', '15'))
MAX_OUTPUT_TOKENS = int(os.getenv('MAX_OUTPUT_TOKENS', '4096'))
MAX_CHAT_HISTORY = int(os.getenv('MAX_CHAT_HISTORY', '2'))

# Configurazione provider di generazione
GENERATION_PROVIDER = os.getenv('GENERATION_PROVIDER', 'gemini').lower()
GENERATION_MODEL = os.getenv('GENERATION_MODEL', DEFAULT_MODEL)
GENERATION_API_KEY = os.getenv('GENERATION_API_KEY', GEMINI_API_KEY)
BASE_URL = 'https://generativelanguage.googleapis.com/v1beta'
UPLOAD_BASE_URL = 'https://generativelanguage.googleapis.com/upload/v1beta'

# Dimensione massima file: 100MB
app.config['MAX_CONTENT_LENGTH'] = 100 * 1024 * 1024
# Upload folder temporaneo
app.config['UPLOAD_FOLDER'] = tempfile.gettempdir()
# Cartella per archiviare i documenti caricati (per download futuro)
app.config['DOCUMENTS_STORAGE'] = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'documents_storage')
# Crea la cartella se non esiste
os.makedirs(app.config['DOCUMENTS_STORAGE'], exist_ok=True)

# MIME types permessi (solo documenti testuali - File Search Store non supporta immagini)
ALLOWED_MIME_TYPES = {
    'application/pdf', 'text/plain', 'text/html', 'text/markdown',
    'application/msword', 'application/vnd.openxmlformats-officedocument.wordprocessingml.document',
    'application/vnd.ms-excel', 'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
    'text/csv', 'application/json'
}

# Cache in-memory per query con TTL
class QueryCache:
    """Cache semplice in-memory per risultati query con Time-To-Live"""
    def __init__(self, ttl_seconds=300):  # Default: 5 minuti
        self.cache = {}
        self.ttl = ttl_seconds
    
    def get(self, key):
        """Recupera valore dalla cache se non scaduto"""
        if key in self.cache:
            value, timestamp = self.cache[key]
            if time.time() - timestamp < self.ttl:
                logger.debug(f"Cache HIT per query: {key[:50]}...")
                return value
            else:
                # Scaduto, rimuovi
                del self.cache[key]
                logger.debug(f"Cache EXPIRED per query: {key[:50]}...")
        return None
    
    def set(self, key, value):
        """Memorizza valore in cache con timestamp"""
        self.cache[key] = (value, time.time())
        logger.debug(f"Cache SET per query: {key[:50]}...")
    
    def clear(self):
        """Svuota cache"""
        self.cache.clear()
        logger.info("Cache svuotata")
    
    def size(self):
        """Ritorna numero elementi in cache"""
        return len(self.cache)

# Inizializza cache globale
query_cache = QueryCache(ttl_seconds=int(os.getenv('QUERY_CACHE_TTL', '300')))

# Rate limiter in-memory
class RateLimiter:
    """Rate limiter semplice basato su token bucket"""
    def __init__(self, max_requests=10, time_window=60):
        self.max_requests = max_requests
        self.time_window = time_window  # secondi
        self.requests = {}  # {ip: [(timestamp, ...), ...]}
    
    def is_allowed(self, identifier):
        """Verifica se la richiesta è permessa"""
        now = time.time()
        
        # Pulisci richieste vecchie
        if identifier in self.requests:
            self.requests[identifier] = [
                ts for ts in self.requests[identifier]
                if now - ts < self.time_window
            ]
        else:
            self.requests[identifier] = []
        
        # Controlla limite
        if len(self.requests[identifier]) >= self.max_requests:
            logger.warning(f"Rate limit exceeded per {identifier}")
            return False
        
        # Aggiungi nuova richiesta
        self.requests[identifier].append(now)
        return True
    
    def get_remaining(self, identifier):
        """Ritorna richieste rimanenti"""
        now = time.time()
        if identifier not in self.requests:
            return self.max_requests
        
        recent = [ts for ts in self.requests[identifier] if now - ts < self.time_window]
        return max(0, self.max_requests - len(recent))

# Inizializza rate limiter
rate_limiter = RateLimiter(
    max_requests=int(os.getenv('RATE_LIMIT_MAX', '30')),
    time_window=int(os.getenv('RATE_LIMIT_WINDOW', '60'))
)

# Session requests per connection pooling
http_session = requests.Session()
adapter = requests.adapters.HTTPAdapter(
    pool_connections=10,
    pool_maxsize=20,
    max_retries=3
)
http_session.mount('https://', adapter)
http_session.mount('http://', adapter)

# Circuit Breaker per gestione rate limit
class CircuitBreaker:
    """Circuit breaker per gestire rate limit 429"""
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_threshold = failure_threshold
        self.timeout = timeout  # secondi
        self.failure_count = 0
        self.last_failure_time: Optional[datetime] = None
        self.state = 'CLOSED'  # CLOSED, OPEN, HALF_OPEN
    
    def call_allowed(self) -> bool:
        """Verifica se la chiamata è permessa"""
        if self.state == 'CLOSED':
            return True
        
        if self.state == 'OPEN':
            # Verifica se è passato il timeout
            if self.last_failure_time and datetime.now() - self.last_failure_time > timedelta(seconds=self.timeout):
                self.state = 'HALF_OPEN'
                logger.info("Circuit breaker: OPEN -> HALF_OPEN (timeout scaduto)")
                return True
            return False
        
        # HALF_OPEN: permette una chiamata di test
        return True
    
    def record_success(self):
        """Registra una chiamata riuscita"""
        self.failure_count = 0
        if self.state == 'HALF_OPEN':
            self.state = 'CLOSED'
            logger.info("Circuit breaker: HALF_OPEN -> CLOSED (successo)")
    
    def record_failure(self):
        """Registra una chiamata fallita"""
        self.failure_count += 1
        self.last_failure_time = datetime.now()
        
        if self.failure_count >= self.failure_threshold:
            self.state = 'OPEN'
            logger.warning(f"Circuit breaker: OPEN (troppi errori: {self.failure_count})")

# Istanza globale del circuit breaker
gemini_circuit_breaker = CircuitBreaker(failure_threshold=5, timeout=60)

def get_headers():
    """Restituisce gli headers per le richieste API"""
    return {
        'x-goog-api-key': GEMINI_API_KEY
    }

def fix_encoding_issues(text: str) -> str:
    """
    Corregge problemi comuni di encoding UTF-8 mal interpretato come Latin-1
    """
    if not text:
        return text
    
    # Mappa dei caratteri comuni mal codificati
    replacements = {
        'â¬': '€',     # Euro
        'Ã ': 'à',     # a con accento grave
        'Ã¨': 'è',     # e con accento grave
        'Ã©': 'é',     # e con accento acuto
        'Ã¬': 'ì',     # i con accento grave
        'Ã²': 'ò',     # o con accento grave
        'Ã¹': 'ù',     # u con accento grave
        'Ã': 'À',      # A con accento grave
        'Ã': 'È',      # E con accento grave
        'Ã': 'É',      # E con accento acuto
        'Ã': 'Ì',      # I con accento grave
        'Ã': 'Ò',      # O con accento grave
        'Ã': 'Ù',      # U con accento grave
        'â': '"',      # Virgolette
        'â': '"',      # Virgolette
        'â': "'",      # Apostrofo
        'â¦': '...',   # Ellipsis
        'â': '–',      # En dash
        'â': '—',      # Em dash
    }
    
    for wrong, correct in replacements.items():
        text = text.replace(wrong, correct)
    
    return text

def validate_query_text(query: str) -> tuple[bool, Optional[str]]:
    """
    Valida il testo della query
    Returns: (is_valid, error_message)
    """
    if not query or not query.strip():
        return False, "Query vuota"
    
    if len(query) > 2000:
        return False, "Query troppo lunga (max 2000 caratteri)"
    
    # Controllo caratteri pericolosi per injection
    dangerous_chars = ['<script', 'javascript:', 'onerror=', 'onclick=']
    query_lower = query.lower()
    for char in dangerous_chars:
        if char in query_lower:
            return False, f"Query contiene contenuto non permesso: {char}"
    
    return True, None

def validate_mime_type(mime_type: str, filename: str) -> tuple[bool, Optional[str]]:
    """
    Valida il MIME type del file
    Returns: (is_valid, error_message)
    """
    if not mime_type:
        # Inferisci dal filename
        mime_type, _ = mimetypes.guess_type(filename)
    
    if not mime_type:
        return False, "Impossibile determinare il tipo di file"
    
    if mime_type not in ALLOWED_MIME_TYPES:
        return False, f"Tipo di file non permesso: {mime_type}. Permessi: {', '.join(ALLOWED_MIME_TYPES)}"
    
    return True, None

def validate_metadata(metadata_keys: list, metadata_values: list) -> tuple[bool, Optional[str]]:
    """
    Valida i metadati custom (ignora righe vuote)
    Returns: (is_valid, error_message)
    """
    if len(metadata_keys) != len(metadata_values):
        return False, "Numero di chiavi e valori metadati non corrispondono"
    
    # Filtra solo metadati non vuoti per la validazione
    non_empty_pairs = [(k, v) for k, v in zip(metadata_keys, metadata_values) if k.strip() or v.strip()]
    
    if len(non_empty_pairs) > 50:
        return False, "Troppi metadati (max 50)"
    
    for key, value in non_empty_pairs:
        # Ignora coppie completamente vuote
        if not key.strip() and not value.strip():
            continue
            
        # Se la chiave è vuota ma il valore no, errore
        if not key.strip() and value.strip():
            return False, "Valore metadato senza chiave associata"
        
        # Valida lunghezza chiave
        if len(key) > 100:
            return False, f"Chiave metadato troppo lunga (max 100 caratteri): {key[:20]}..."
        
        # Controllo caratteri pericolosi nella chiave
        if any(c in key for c in ['<', '>', '"', "'", ';', '\n', '\r']):
            return False, f"Chiave metadato contiene caratteri non permessi: {key}"
        
        # Valida lunghezza valore
        if len(value) > 500:
            return False, f"Valore metadato troppo lungo (max 500 caratteri)"
    
    return True, None

def validate_config():
    """Valida la configurazione dell'applicazione"""
    if not GEMINI_API_KEY:
        raise ValueError("GEMINI_API_KEY non configurata")
    if not FILE_SEARCH_STORE_NAME:
        raise ValueError("FILE_SEARCH_STORE_NAME non configurato")
    logger.info(f"Configurazione valida. Store: {FILE_SEARCH_STORE_NAME}")

def call_generation_provider(messages: list, max_tokens: int = None, temperature: float = 0.7) -> str:
    """
    Chiama il provider di generazione configurato (Gemini, DeepSeek, OpenAI, etc.)
    
    Args:
        messages: Lista di messaggi in formato [{'role': 'user'|'assistant', 'content': 'text'}]
        max_tokens: Numero massimo di token nella risposta
        temperature: Temperatura per la generazione
    
    Returns:
        Il testo della risposta generata
    """
    max_tokens = max_tokens or MAX_OUTPUT_TOKENS
    
    if GENERATION_PROVIDER == 'deepseek':
        # DeepSeek API (compatibile OpenAI)
        import openai
        client = openai.OpenAI(
            api_key=GENERATION_API_KEY,
            base_url="https://api.deepseek.com"
        )
        
        # Converti formato messaggi
        deepseek_messages = []
        for msg in messages:
            deepseek_messages.append({
                'role': msg.get('role'),
                'content': msg.get('content', '')
            })
        
        response = client.chat.completions.create(
            model=GENERATION_MODEL,
            messages=deepseek_messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        return response.choices[0].message.content
    
    elif GENERATION_PROVIDER == 'openai':
        # OpenAI API
        import openai
        client = openai.OpenAI(api_key=GENERATION_API_KEY)
        
        openai_messages = []
        for msg in messages:
            openai_messages.append({
                'role': msg.get('role'),
                'content': msg.get('content', '')
            })
        
        response = client.chat.completions.create(
            model=GENERATION_MODEL,
            messages=openai_messages,
            max_tokens=max_tokens,
            temperature=temperature
        )
        
        return response.choices[0].message.content
    
    else:  # default: gemini
        # Gemini API (logica esistente)
        contents = []
        for msg in messages:
            contents.append({
                'role': msg.get('role'),
                'parts': [{'text': msg.get('content', '')}]
            })
        
        generate_url = f"{BASE_URL}/models/{GENERATION_MODEL}:generateContent"
        payload = {
            'contents': contents,
            'generationConfig': {
                'temperature': temperature,
                'topK': 40,
                'topP': 0.95,
                'maxOutputTokens': max_tokens,
            }
        }
        
        response = http_session.post(generate_url, headers=get_headers(), json=payload, timeout=60)
        response.raise_for_status()
        
        result = response.json()
        candidates = result.get('candidates', [])
        if not candidates:
            raise ValueError('Nessuna risposta generata dal modello')
        
        return candidates[0].get('content', {}).get('parts', [{}])[0].get('text', '')

@app.route('/')
def index():
    """Pagina principale dell'interfaccia amministrativa"""
    return render_template('index.html')

@app.route('/api/config', methods=['GET'])
def get_config():
    """Restituisce la configurazione corrente (senza API key)"""
    try:
        return jsonify({
            'success': True,
            'store_name': FILE_SEARCH_STORE_NAME,
            'api_configured': bool(GEMINI_API_KEY),
            'chunk_size': DEFAULT_CHUNK_SIZE,
            'chunk_overlap_percent': CHUNK_OVERLAP_PERCENT,
            'results_count': RESULTS_COUNT,
            'min_relevance_score': MIN_RELEVANCE_SCORE,
            'max_chunks_for_generation': MAX_CHUNKS_FOR_GENERATION
        })
    except Exception as e:
        logger.error(f"Errore nel recupero configurazione: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/documents', methods=['GET'])
def list_documents():
    """Elenca tutti i documenti nel File Search Store"""
    try:
        url = f"{BASE_URL}/{FILE_SEARCH_STORE_NAME}/documents"
        headers = get_headers()
        
        # Parametri opzionali per paginazione (max 20 per Google API)
        page_size = min(int(request.args.get('pageSize', 20)), 20)
        page_token = request.args.get('pageToken', '')
        
        params = {'pageSize': page_size}
        if page_token:
            params['pageToken'] = page_token
        
        logger.info(f"Recupero documenti da: {url}")
        response = http_session.get(url, headers=headers, params=params)
        response.raise_for_status()
        
        data = response.json()
        documents = data.get('documents', [])
        
        logger.info(f"Recuperati {len(documents)} documenti")
        
        return jsonify({
            'success': True,
            'documents': documents,
            'nextPageToken': data.get('nextPageToken', '')
        })
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Errore nella lista documenti: {str(e)}")
        error_detail = e.response.json() if hasattr(e, 'response') and e.response.content else str(e)
        return jsonify({
            'success': False,
            'error': 'Errore nel recupero dei documenti',
            'details': error_detail
        }), 500
    except Exception as e:
        logger.error(f"Errore imprevisto: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/documents/upload', methods=['POST'])
def upload_document():
    """Carica un documento nel File Search Store (Long-Running Operation)"""
    temp_file_path = None
    try:
        # Verifica presenza file
        if 'file' not in request.files:
            return jsonify({'success': False, 'error': 'Nessun file fornito'}), 400
        
        file = request.files['file']
        if file.filename == '':
            return jsonify({'success': False, 'error': 'Nome file vuoto'}), 400
        
        # Recupera metadati opzionali
        display_name = request.form.get('displayName', file.filename)
        mime_type = request.form.get('mimeType', '')
        chunk_size = int(request.form.get('chunkSize', DEFAULT_CHUNK_SIZE))  # Default dal .env
        
        # VALIDAZIONE CHUNK SIZE (limite API Google: 1-512)
        if chunk_size < 1 or chunk_size > 512:
            return jsonify({'success': False, 'error': f'Chunk size deve essere tra 1 e 512 (ricevuto: {chunk_size})'}), 400
        
        # Inferisci MIME type se non fornito
        if not mime_type:
            mime_type = mimetypes.guess_type(file.filename)[0] or 'application/octet-stream'
        
        # VALIDAZIONE MIME TYPE
        is_valid, error = validate_mime_type(mime_type, file.filename)
        if not is_valid:
            return jsonify({'success': False, 'error': error}), 400
        
        # Metadati custom (opzionale)
        custom_metadata = {}
        metadata_keys = request.form.getlist('metadataKeys[]')
        metadata_values = request.form.getlist('metadataValues[]')
        
        logger.info(f"Metadati ricevuti - Keys: {metadata_keys}, Values: {metadata_values}")
        
        # VALIDAZIONE METADATI
        if metadata_keys or metadata_values:
            is_valid, error = validate_metadata(metadata_keys, metadata_values)
            if not is_valid:
                logger.error(f"Validazione metadati fallita: {error}")
                return jsonify({'success': False, 'error': error}), 400
        
        # Filtra e aggiungi solo metadati non vuoti
        for key, value in zip(metadata_keys, metadata_values):
            if key and key.strip() and value and value.strip():
                custom_metadata[key.strip()] = value.strip()
        
        logger.info(f"Metadati custom validati: {custom_metadata}")
        
        # SALVA FILE TEMPORANEAMENTE SU DISCO (non in memoria)
        temp_file_path = os.path.join(app.config['UPLOAD_FOLDER'], f"upload_{int(time.time())}_{secure_filename(file.filename)}")
        file.save(temp_file_path)
        logger.info(f"File salvato temporaneamente: {temp_file_path}")
        
        # URL per upload
        url = f"{UPLOAD_BASE_URL}/{FILE_SEARCH_STORE_NAME}:uploadToFileSearchStore"
        
        headers = get_headers()
        
        # Prepara i metadati del documento come JSON
        metadata = {
            'displayName': display_name.strip()[:100],  # Max 100 caratteri e senza spazi extra
            'mimeType': mime_type
        }
        
        if custom_metadata:
            metadata['customMetadata'] = [
                {'key': k, 'stringValue': v} for k, v in custom_metadata.items()
            ]
        
        # CONFIGURAZIONE CHUNKING per divisione ottimale del documento
        # La configurazione segue la struttura corretta dell'API Google
        chunk_overlap = min(int(chunk_size * CHUNK_OVERLAP_PERCENT / 100), 100)  # Overlap dal .env, max 100
        
        metadata['chunkingConfig'] = {
            'whiteSpaceConfig': {
                'maxTokensPerChunk': chunk_size,      # Dimensione massima chunk in token
                'maxOverlapTokens': chunk_overlap     # Token di sovrapposizione tra chunks
            }
        }
        
        logger.info(f"Chunking config: max_tokens={chunk_size}, overlap={chunk_overlap}")
        
        # Apri il file salvato e invialo (non usare stream che carica in memoria)
        with open(temp_file_path, 'rb') as f:
            files = {
                'metadata': (None, json.dumps(metadata), 'application/json'),
                'file': (secure_filename(file.filename), f, mime_type)
            }
            
            logger.info(f"Caricamento file: {file.filename} ({mime_type})")
            logger.info(f"Display name: {display_name}")
            logger.info(f"Metadata: {json.dumps(metadata)}")
            
            # Effettua l'upload - restituisce un'operazione
            response = http_session.post(url, headers=headers, files=files)
            response.raise_for_status()
        
        operation_data = response.json()
        operation_name = operation_data.get('name', '')
        
        logger.info(f"Upload avviato. Operation: {operation_name}")
        
        return jsonify({
            'success': True,
            'operation': operation_data,
            'operationName': operation_name,
            'message': 'Upload avviato con successo. L\'elaborazione è in corso.'
        })
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Errore durante upload: {str(e)}")
        error_detail = str(e)
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_detail = e.response.json()
                logger.error(f"Dettagli errore API: {error_detail}")
            except:
                error_detail = e.response.text
                logger.error(f"Risposta errore API (testo): {error_detail}")
        return jsonify({
            'success': False,
            'error': 'Errore durante il caricamento',
            'details': error_detail
        }), 500
    except Exception as e:
        logger.error(f"Errore imprevisto durante upload: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500
    finally:
        # PULIZIA FILE TEMPORANEO (anche se ci sono stati errori)
        if temp_file_path and os.path.exists(temp_file_path):
            try:
                os.remove(temp_file_path)
                logger.info(f"File temporaneo eliminato: {temp_file_path}")
            except Exception as cleanup_error:
                logger.warning(f"Errore nella pulizia del file temporaneo: {cleanup_error}")

@app.route('/api/operations/<path:operation_name>', methods=['GET'])
def get_operation_status(operation_name):
    """Recupera lo stato di un'operazione di upload"""
    try:
        # Validazione
        if not operation_name or operation_name == 'undefined':
            logger.error("Operation name mancante o undefined")
            return jsonify({
                'success': False,
                'error': 'Operation name mancante'
            }), 400
        
        # L'operation name è già completo (es: fileSearchStores/.../upload/operations/...)
        url = f"{BASE_URL}/{operation_name}"
        headers = get_headers()
        
        logger.info(f"Controllo stato operazione: {operation_name}")
        
        response = http_session.get(url, headers=headers)
        response.raise_for_status()
        
        operation_data = response.json()
        done = operation_data.get('done', False)
        
        result = {
            'success': True,
            'operation': operation_data,
            'done': done
        }
        
        if done:
            if 'error' in operation_data:
                result['error'] = operation_data['error']
                logger.warning(f"Operazione completata con errore: {operation_data['error']}")
            else:
                result['document'] = operation_data.get('response', {})
                logger.info(f"Operazione completata con successo")
        
        return jsonify(result)
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Errore nel controllo operazione: {str(e)}")
        error_detail = e.response.json() if hasattr(e, 'response') and e.response.content else str(e)
        return jsonify({
            'success': False,
            'error': 'Errore nel controllo dello stato',
            'details': error_detail
        }), 500
    except Exception as e:
        logger.error(f"Errore imprevisto: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/documents/<path:document_name>', methods=['DELETE'])
def delete_document(document_name):
    """Elimina un documento dal File Search Store"""
    try:
        # Il document_name include già il path completo (fileSearchStores/.../documents/...)
        url = f"{BASE_URL}/{document_name}"
        headers = get_headers()
        
        # IMPORTANTE: force=true elimina anche tutti i Chunk associati
        params = {'force': 'true'}
        
        logger.info(f"Eliminazione documento: {document_name}")
        
        response = http_session.delete(url, headers=headers, params=params)
        response.raise_for_status()
        
        logger.info(f"Documento eliminato con successo")
        
        return jsonify({
            'success': True,
            'message': 'Documento eliminato con successo'
        })
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Errore durante eliminazione: {str(e)}")
        error_detail = e.response.json() if hasattr(e, 'response') and e.response.content else str(e)
        return jsonify({
            'success': False,
            'error': 'Errore durante l\'eliminazione',
            'details': error_detail
        }), 500
    except Exception as e:
        logger.error(f"Errore imprevisto: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

# ==================== CHATBOT ENDPOINTS ====================

@app.route('/api/chat/query', methods=['POST'])
def query_documents():
    """
    Endpoint per eseguire una query sui documenti (Retrieval Phase)
    Recupera i chunk rilevanti dai documenti attivi
    """
    try:
        # Rate limiting
        client_ip = request.remote_addr
        if not rate_limiter.is_allowed(client_ip):
            return jsonify({
                'success': False,
                'error': 'Troppe richieste. Riprova tra un minuto.'
            }), 429
        
        data = request.json
        query_text = data.get('query')
        document_name = data.get('document_name')  # Opzionale: nome specifico del documento
        results_count = data.get('results_count', RESULTS_COUNT)  # Default da .env, max 100
        
        if not query_text:
            return jsonify({
                'success': False,
                'error': 'Query text è obbligatorio'
            }), 400
        
        # VALIDAZIONE INPUT
        is_valid, error = validate_query_text(query_text)
        if not is_valid:
            return jsonify({'success': False, 'error': error}), 400
        
        logger.info(f"Query ricevuta: {query_text}")
        
        # Cache check
        cache_key = f"{query_text}:{document_name}:{results_count}"
        cached_result = query_cache.get(cache_key)
        if cached_result:
            logger.info("Risultato recuperato da cache")
            return jsonify(cached_result)
        
        # Se non è specificato un documento, cerchiamo in tutti i documenti attivi
        if not document_name:
            # Prima otteniamo la lista dei documenti attivi
            list_url = f"{BASE_URL}/{FILE_SEARCH_STORE_NAME}/documents"
            list_response = http_session.get(list_url, headers=get_headers())
            list_response.raise_for_status()
            
            documents_data = list_response.json()
            documents = documents_data.get('documents', [])
            
            # Filtra solo documenti attivi
            active_documents = [doc for doc in documents if doc.get('state') == 'STATE_ACTIVE']
            
            if not active_documents:
                return jsonify({
                    'success': False,
                    'error': 'Nessun documento attivo trovato'
                }), 404
            
            # Calcola chunks per documento: distribuisci RESULTS_COUNT tra i documenti
            # Con 25 chunks e 5 documenti = 5 chunks per documento
            # Con 25 chunks e 10 documenti = 3 chunks per documento (arrotonda up)
            chunks_per_document = max(3, (results_count + len(active_documents) - 1) // len(active_documents))
            logger.info(f"Query su {len(active_documents)} documenti attivi, {chunks_per_document} chunks per documento")
            
            # Interroga tutti i documenti attivi e aggrega i risultati
            all_chunks = []
            for doc in active_documents:
                doc_name = doc.get('name')
                query_url = f"{BASE_URL}/{doc_name}:query"
                
                query_payload = {
                    'query': query_text,
                    'resultsCount': chunks_per_document
                }
                
                try:
                    query_response = http_session.post(query_url, headers=get_headers(), json=query_payload)
                    query_response.raise_for_status()
                    
                    result = query_response.json()
                    chunks = result.get('relevantChunks', [])
                    
                    # Aggiungi informazioni sul documento sorgente
                    for chunk in chunks:
                        chunk['source_document'] = doc.get('displayName', doc_name)
                    
                    all_chunks.extend(chunks)
                    logger.info(f"  - {doc.get('displayName')}: {len(chunks)} chunks recuperati")
                except Exception as e:
                    logger.warning(f"Errore query su documento {doc_name}: {str(e)}")
                    continue
            
            # Ordina per rilevanza (assumendo che abbiano un campo score)
            all_chunks.sort(key=lambda x: x.get('chunkRelevanceScore', 0), reverse=True)
            
            # Limita al numero richiesto
            all_chunks = all_chunks[:results_count]
            
            logger.info(f"Totale chunks aggregati: {len(all_chunks)}, top score: {all_chunks[0].get('chunkRelevanceScore', 0):.2f} se disponibile")
            
            result_data = {
                'success': True,
                'relevant_chunks': all_chunks,
                'query': query_text,
                'documents_searched': len(active_documents)
            }
            
            # Salva in cache
            query_cache.set(cache_key, result_data)
            
            return jsonify(result_data)
        
        else:
            # Query su un documento specifico
            query_url = f"{BASE_URL}/{document_name}:query"
            
            query_payload = {
                'query': query_text,
                'resultsCount': results_count
            }
            
            query_response = http_session.post(query_url, headers=get_headers(), json=query_payload)
            query_response.raise_for_status()
            
            result = query_response.json()
            
            return jsonify({
                'success': True,
                'relevant_chunks': result.get('relevantChunks', []),
                'query': query_text
            })
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Errore durante query: {str(e)}")
        error_detail = e.response.json() if hasattr(e, 'response') and e.response.content else str(e)
        return jsonify({
            'success': False,
            'error': 'Errore durante la query',
            'details': error_detail
        }), 500
    except Exception as e:
        logger.error(f"Errore imprevisto: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/chat/generate', methods=['POST'])
def generate_response():
    """
    Endpoint per generare una risposta usando Gemini (Generation Phase)
    Prende i chunk rilevanti e genera una risposta coerente
    """
    try:
        data = request.json
        query_text = data.get('query')
        relevant_chunks = data.get('relevant_chunks', [])
        chat_history = data.get('chat_history', [])  # Per conversazioni multi-turn
        model = data.get('model', DEFAULT_MODEL)  # Default dal .env
        
        if not query_text:
            return jsonify({
                'success': False,
                'error': 'Query text è obbligatorio'
            }), 400
        
        # VALIDAZIONE INPUT
        is_valid, error = validate_query_text(query_text)
        if not is_valid:
            return jsonify({'success': False, 'error': error}), 400
        
        # CONTROLLO CIRCUIT BREAKER
        if not gemini_circuit_breaker.call_allowed():
            logger.warning("Circuit breaker APERTO - troppe richieste fallite a Gemini API")
            return jsonify({
                'success': False,
                'error': 'Servizio temporaneamente non disponibile. Riprova tra qualche minuto.',
                'circuit_breaker_status': 'OPEN'
            }), 503
        
        logger.info(f"Generazione risposta per: {query_text}")
        
        # Filtra chunk per generazione basandoci sulla rilevanza
        # Usa solo chunk con score alto o limita il numero
        high_score_chunks = [
            chunk for chunk in relevant_chunks 
            if chunk.get('chunkRelevanceScore', 0) >= MIN_RELEVANCE_SCORE
        ]
        
        # Se non ci sono chunk con score alto, usa comunque i migliori disponibili
        if not high_score_chunks and relevant_chunks:
            logger.warning(f"Nessun chunk supera MIN_RELEVANCE_SCORE={MIN_RELEVANCE_SCORE}, uso i migliori {MAX_CHUNKS_FOR_GENERATION} disponibili")
            chunks_to_use = relevant_chunks[:MAX_CHUNKS_FOR_GENERATION]
        else:
            # Se abbiamo troppi chunk anche dopo il filtro, prendi i top N
            chunks_to_use = high_score_chunks[:MAX_CHUNKS_FOR_GENERATION]
        
        logger.info(f"Chunk recuperati: {len(relevant_chunks)}, Score >= {MIN_RELEVANCE_SCORE}: {len(high_score_chunks)}, Usati per generazione: {len(chunks_to_use)}")
        
        # Costruisci il contesto dai chunk rilevanti
        context_parts = []
        if chunks_to_use:
            context_parts.append("CONTESTO DOCUMENTI:\n\n")
            for i, chunk in enumerate(chunks_to_use, 1):
                chunk_text = chunk.get('chunk', {}).get('data', {}).get('stringValue', '')
                source = chunk.get('source_document', 'documento')
                context_parts.append(f"[Frammento {i} da {source}]:\n{chunk_text}\n\n")
        
        # System instruction compatto
        system_instruction = """Sei un assistente AI che risponde basandosi SOLO sui documenti forniti. 
Rispondi in modo chiaro, conciso ed estrai solo le informazioni rilevanti."""
        
        # STRATEGIA OTTIMIZZATA: Invia solo le DOMANDE dell'utente (non le risposte)
        # Estrai solo le domande dell'utente dalla cronologia
        user_questions = [msg.get('text', '') for msg in chat_history if msg.get('role') == 'user']
        
        # Limita al numero massimo configurato
        max_history_messages = MAX_CHAT_HISTORY * 2
        recent_questions = user_questions[-max_history_messages:] if len(user_questions) > max_history_messages else user_questions
        
        # Rimuovi duplicati
        unique_questions = []
        seen = set()
        for q in recent_questions:
            if q and q not in seen:
                unique_questions.append(q)
                seen.add(q)
        
        # Costruisci un singolo prompt
        user_prompt = system_instruction + "\n\n"
        
        # Aggiungi il contesto dei documenti
        user_prompt += ''.join(context_parts)
        
        # Aggiungi cronologia domande precedenti (se ci sono)
        if unique_questions:
            user_prompt += "\n\nCONTESTO CONVERSAZIONE - Domande precedenti dell'utente:\n"
            for i, q in enumerate(unique_questions, 1):
                user_prompt += f"{i}. {q}\n"
            user_prompt += "\n"
        
        # Aggiungi la domanda corrente
        user_prompt += f"DOMANDA CORRENTE: {query_text}\n\nRISPOSTA:"
        
        # Costruisci contents con un singolo messaggio user
        contents = [{
            'role': 'user',
            'parts': [{'text': user_prompt}]
        }]
        
        # Chiamata all'API Gemini
        generate_url = f"{BASE_URL}/models/{model}:generateContent"
        
        payload = {
            'contents': contents,
            'generationConfig': {
                'temperature': 0.7,
                'topK': 40,
                'topP': 0.95,
                'maxOutputTokens': 8192,  # Aumentato per risposte più lunghe (era 2048)
            }
        }
        
        # Esegui la chiamata al modello con retries su 429 (rate limit)
        max_retries = 3
        delay = 1
        response = None
        for attempt in range(max_retries):
            try:
                response = http_session.post(generate_url, headers=get_headers(), json=payload, timeout=60)
                response.raise_for_status()
                
                # REGISTRA SUCCESSO NEL CIRCUIT BREAKER
                gemini_circuit_breaker.record_success()
                break
            except requests.exceptions.HTTPError as he:
                status = he.response.status_code if he.response is not None else None
                # Se riceviamo 429 (Too Many Requests), ritentiamo con backoff
                if status == 429:
                    # REGISTRA FALLIMENTO NEL CIRCUIT BREAKER
                    gemini_circuit_breaker.record_failure()
                    
                    if attempt < max_retries - 1:
                        logger.warning(f"429 from Gemini API, retry {attempt+1}/{max_retries} after {delay}s")
                        time.sleep(delay)
                        delay *= 2
                        continue
                # Non gestito qui: rilancia per essere catturato più in basso
                raise
        
        if response is None:
            raise RuntimeError('Nessuna risposta dal servizio di generazione')

        result = response.json()
        
        # Estrai il testo della risposta
        candidates = result.get('candidates', [])
        if not candidates:
            return jsonify({
                'success': False,
                'error': 'Nessuna risposta generata dal modello'
            }), 500
        
        response_text = candidates[0].get('content', {}).get('parts', [{}])[0].get('text', '')
        
        # Correggi problemi di encoding
        response_text = fix_encoding_issues(response_text)
        
        return jsonify({
            'success': True,
            'response': response_text,
            'query': query_text,
            'model': model,
            'chunks_used': len(chunks_to_use),
            'chunks_filtered': chunks_to_use  # Restituisce solo i chunks effettivamente usati
        })
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Errore durante generazione: {str(e)}")
        # Se l'errore proviene dall'API esterna, proviamo ad estrarre lo status code
        status_code = None
        try:
            status_code = e.response.status_code
        except Exception:
            status_code = None

        error_detail = None
        try:
            error_detail = e.response.json() if hasattr(e, 'response') and e.response.content else str(e)
        except Exception:
            error_detail = str(e)

        # Se l'API esterna ha restituito 429 (rate limit), inoltriamo 429 al client con dettagli
        if status_code == 429:
            return jsonify({
                'success': False,
                'error': 'Rate limit raggiunto presso il servizio di generazione (429)',
                'details': error_detail
            }), 429

        return jsonify({
            'success': False,
            'error': 'Errore durante la generazione della risposta',
            'details': error_detail
        }), 500
    except Exception as e:
        logger.error(f"Errore imprevisto: {str(e)}")
        return jsonify({'success': False, 'error': str(e)}), 500

@app.route('/api/chat/generate-stream', methods=['POST'])
def generate_response_stream():
    """
    Endpoint per generare una risposta in streaming usando Gemini
    Invia i chunk di testo man mano che vengono generati (SSE)
    """
    data = request.json
    query_text = data.get('query')
    relevant_chunks = data.get('relevant_chunks', [])
    chat_history = data.get('chat_history', [])
    model = data.get('model', DEFAULT_MODEL)  # Default dal .env
    
    # Validazione input
    if not query_text:
        return jsonify({'success': False, 'error': 'Query text è obbligatorio'}), 400
    
    is_valid, error = validate_query_text(query_text)
    if not is_valid:
        return jsonify({'success': False, 'error': error}), 400
    
    # Controllo circuit breaker
    if not gemini_circuit_breaker.call_allowed():
        return jsonify({
            'success': False,
            'error': 'Servizio temporaneamente non disponibile. Riprova tra qualche minuto.',
            'circuit_breaker_status': 'OPEN'
        }), 503
    
    def generate():
        """Generatore per lo streaming SSE"""
        try:
            # Filtra chunk per generazione basandoci sulla rilevanza
            high_score_chunks = [
                chunk for chunk in relevant_chunks 
                if chunk.get('chunkRelevanceScore', 0) >= MIN_RELEVANCE_SCORE
            ]
            
            # Se non ci sono chunk con score alto, usa comunque i migliori disponibili
            if not high_score_chunks and relevant_chunks:
                logger.warning(f"Streaming - Nessun chunk supera MIN_RELEVANCE_SCORE={MIN_RELEVANCE_SCORE}, uso i migliori {MAX_CHUNKS_FOR_GENERATION} disponibili")
                chunks_to_use = relevant_chunks[:MAX_CHUNKS_FOR_GENERATION]
            else:
                chunks_to_use = high_score_chunks[:MAX_CHUNKS_FOR_GENERATION]
            
            logger.info(f"Streaming - Chunk recuperati: {len(relevant_chunks)}, Score >= {MIN_RELEVANCE_SCORE}: {len(high_score_chunks)}, Usati: {len(chunks_to_use)}")
            
            # Debug: mostra la struttura del primo chunk
            if chunks_to_use:
                logger.info(f"Esempio chunk per streaming: {json.dumps(chunks_to_use[0], indent=2)[:500]}")
            
            # Costruisci il contesto
            context_parts = []
            if chunks_to_use:
                context_parts.append("CONTESTO DOCUMENTI:\n\n")
                for i, chunk in enumerate(chunks_to_use, 1):
                    # Supporta entrambi i formati: nidificato e piatto
                    chunk_text = chunk.get('chunk', {}).get('data', {}).get('stringValue', '') or chunk.get('stringValue', '')
                    source = chunk.get('source_document', 'documento')
                    if chunk_text:
                        context_parts.append(f"[Frammento {i} da {source}]:\n{chunk_text}\n\n")
                        logger.debug(f"Chunk {i}: {len(chunk_text)} caratteri")
            
            logger.info(f"Contesto costruito: {len(context_parts)} parti, totale {sum(len(p) for p in context_parts)} caratteri")
            
            # System instruction compatto
            system_instruction = """Sei un assistente AI che risponde basandosi SOLO sui documenti forniti. 
Rispondi in modo chiaro, conciso ed estrai solo le informazioni rilevanti."""
            
            # STRATEGIA OTTIMIZZATA: Invia solo le DOMANDE dell'utente (non le risposte)
            # Questo riduce drasticamente i token usati mantenendo il contesto della conversazione
            
            # Estrai solo le domande dell'utente dalla cronologia (ignora le risposte dell'assistant)
            user_questions = [msg.get('text', '') for msg in chat_history if msg.get('role') == 'user']
            
            # Limita al numero massimo configurato
            max_history_messages = MAX_CHAT_HISTORY * 2  # Moltiplica per 2 perché contiamo solo user
            recent_questions = user_questions[-max_history_messages:] if len(user_questions) > max_history_messages else user_questions
            
            # Rimuovi duplicati (a volte il frontend invia la stessa domanda 2 volte)
            unique_questions = []
            seen = set()
            for q in recent_questions:
                if q and q not in seen:
                    unique_questions.append(q)
                    seen.add(q)
            
            logger.info(f"Chat history: {len(user_questions)} domande utente totali, usando le ultime {len(unique_questions)} uniche")
            
            # Costruisci un singolo prompt con: system instruction + contesto + domande precedenti + domanda corrente
            user_prompt = system_instruction + "\n\n"
            
            # Aggiungi il contesto dei documenti
            user_prompt += ''.join(context_parts)
            
            # Aggiungi cronologia domande precedenti (se ci sono)
            if unique_questions:
                user_prompt += "\n\nCONTESTO CONVERSAZIONE - Domande precedenti dell'utente:\n"
                for i, q in enumerate(unique_questions, 1):
                    user_prompt += f"{i}. {q}\n"
                user_prompt += "\n"
            
            # Aggiungi la domanda corrente
            user_prompt += f"DOMANDA CORRENTE: {query_text}\n\nRISPOSTA:"
            
            logger.info(f"User prompt totale: {len(user_prompt)} caratteri (contesto: {sum(len(p) for p in context_parts)}, domande: {len(unique_questions)})")
            
            # Costruisci contents con un singolo messaggio user
            contents = [{
                'role': 'user',
                'parts': [{'text': user_prompt}]
            }]
            
            # Chiamata streaming all'API Gemini
            # IMPORTANTE: Aggiungi alt=sse per ricevere Server-Sent Events
            stream_url = f"{BASE_URL}/models/{model}:streamGenerateContent?alt=sse"
            payload = {
                'contents': contents,
                'generationConfig': {
                    'temperature': 0.7,
                    'topK': 40,
                    'topP': 0.95,
                    'maxOutputTokens': 8192,  # Aumentato per risposte più lunghe (era 2048)
                }
            }
            
            # Stream con requests - con retry su 503
            logger.info(f"Chiamata API streaming a {stream_url}")
            logger.info(f"Payload prompt length: {len(user_prompt)} caratteri")
            
            max_retries = 3
            delay = 2
            response = None
            
            for attempt in range(max_retries):
                try:
                    response = http_session.post(stream_url, headers=get_headers(), json=payload, stream=True, timeout=60)
                    logger.info(f"Risposta API status: {response.status_code} (attempt {attempt+1})")
                    
                    # Se riceviamo 503 o 429, ritentiamo
                    if response.status_code in [503, 429]:
                        if attempt < max_retries - 1:
                            logger.warning(f"{response.status_code} from Gemini API, retry {attempt+1}/{max_retries} after {delay}s")
                            response.close()
                            time.sleep(delay)
                            delay *= 2
                            continue
                    
                    response.raise_for_status()
                    gemini_circuit_breaker.record_success()
                    break
                except requests.exceptions.RequestException as e:
                    if attempt < max_retries - 1:
                        logger.warning(f"Request error, retry {attempt+1}/{max_retries} after {delay}s: {str(e)}")
                        time.sleep(delay)
                        delay *= 2
                        continue
                    raise
            
            if response is None:
                raise RuntimeError('Nessuna risposta dal servizio dopo tutti i retry')
            
            chunk_count = 0
            raw_chunk_count = 0
            
            # DEBUG: Leggi il contenuto grezzo per vedere il formato
            logger.info("Inizio lettura streaming...")
            
            # Gemini restituisce SSE format: "data: {...json...}\n"
            for line in response.iter_lines(decode_unicode=True):
                raw_chunk_count += 1
                logger.debug(f"Raw line #{raw_chunk_count}: {line[:200] if line else 'EMPTY'}")
                
                if line and line.startswith('data: '):
                    try:
                        # Rimuovi il prefisso "data: " e parsa il JSON
                        json_str = line[6:]  # Salta "data: "
                        chunk_data = json.loads(json_str)
                        logger.debug(f"Parsed JSON keys: {list(chunk_data.keys())}")
                        
                        # Estrai il testo dal chunk
                        candidates = chunk_data.get('candidates', [])
                        if candidates:
                            candidate = candidates[0]
                            logger.info(f"Candidates trovati: {len(candidates)}, keys: {list(candidate.keys())}")
                            
                            # Controlla se c'è un finishReason
                            if 'finishReason' in candidate:
                                finish_reason = candidate.get('finishReason')
                                logger.warning(f"⚠️ Streaming terminato con finishReason: {finish_reason}")
                                # Invia un messaggio di avviso al frontend se non è STOP normale
                                if finish_reason != 'STOP':
                                    yield f"data: {json.dumps({'warning': f'Risposta incompleta: {finish_reason}'})}\n\n"
                            
                            if 'content' in candidate:
                                parts = candidate.get('content', {}).get('parts', [])
                                logger.info(f"Parts trovati: {len(parts)}")
                                if parts and 'text' in parts[0]:
                                    text_chunk = parts[0]['text']
                                    logger.info(f"Text chunk estratto: {repr(text_chunk[:100])}")
                                    # Invia anche chunk vuoti/whitespace - il frontend li gestirà
                                    if text_chunk:
                                        # Correggi problemi di encoding
                                        text_chunk = fix_encoding_issues(text_chunk)
                                        chunk_count += 1
                                        logger.info(f"✓ Inviato chunk {chunk_count}: {text_chunk[:50]}...")
                                        # Invia il chunk come SSE
                                        yield f"data: {json.dumps({'text': text_chunk})}\n\n"
                                else:
                                    logger.info(f"No text in parts: {parts}")
                            else:
                                logger.info(f"No content in candidate, keys: {list(candidate.keys())}")
                    except (json.JSONDecodeError, IndexError, KeyError) as e:
                        logger.warning(f"Errore parsing chunk streaming: {str(e)}, line: {line[:100]}")
                        continue
            
            logger.info(f"Streaming completato: {raw_chunk_count} raw chunks ricevuti, {chunk_count} chunks testo inviati")
            # Segnala fine dello streaming
            yield f"data: {json.dumps({'done': True})}\n\n"
                
        except requests.exceptions.HTTPError as he:
            if he.response and he.response.status_code == 429:
                gemini_circuit_breaker.record_failure()
            yield f"data: {json.dumps({'error': 'Errore durante la generazione'})}\n\n"
        except Exception as e:
            logger.error(f"Errore streaming: {str(e)}")
            yield f"data: {json.dumps({'error': str(e)})}\n\n"
    
    return Response(stream_with_context(generate()), mimetype='text/event-stream')

@app.route('/chat')
def chat_page():
    """Pagina dell'interfaccia chatbot"""
    return render_template('chat.html')

@app.route('/chunks')
def chunks_page():
    """Pagina per visualizzare i chunks dei documenti"""
    return render_template('chunks.html')

@app.route('/api/documents/<path:document_name>/chunks', methods=['POST'])
def get_document_chunks(document_name):
    """
    Recupera tutti i chunks di un documento specifico tramite query.
    L'API Google non ha un endpoint diretto per listare chunks,
    quindi usiamo il metodo query con una stringa generica.
    
    Body params:
    - query: stringa di ricerca (opzionale, default: "*" per tutti i chunks)
    - resultsCount: numero di chunks da recuperare (max 100)
    """
    try:
        data = request.get_json() or {}
        query_string = data.get('query', '*')  # Query generica per ottenere tutti i chunks
        results_count = min(int(data.get('resultsCount', 100)), 100)  # Max 100 per API
        
        # Costruisci URL per query sui chunks del documento
        url = f"{BASE_URL}/{document_name}:query"
        
        headers = get_headers()
        headers['Content-Type'] = 'application/json'
        
        # Payload per la query
        payload = {
            'query': query_string,
            'resultsCount': results_count
        }
        
        logger.info(f"Query chunks su documento: {document_name}")
        logger.info(f"Query: '{query_string}', Max results: {results_count}")
        
        response = http_session.post(url, headers=headers, json=payload)
        response.raise_for_status()
        
        response_data = response.json()
        relevant_chunks = response_data.get('relevantChunks', [])
        
        logger.info(f"Recuperati {len(relevant_chunks)} chunks")
        
        # Debug: log della struttura del primo chunk
        if relevant_chunks:
            logger.info(f"Esempio chunk: {json.dumps(relevant_chunks[0], indent=2)}")
        
        # Recupera informazioni del documento per includere i metadati
        document_info = None
        try:
            doc_url = f"{BASE_URL}/{document_name}"
            doc_response = http_session.get(doc_url, headers=headers)
            doc_response.raise_for_status()
            document_info = doc_response.json()
            logger.info(f"Metadati documento recuperati: {document_info.get('displayName', 'N/A')}")
        except Exception as doc_error:
            logger.warning(f"Impossibile recuperare metadati documento: {doc_error}")
        
        # Formatta i chunks mantenendo la struttura originale per il frontend
        formatted_chunks = []
        for chunk_wrapper in relevant_chunks:
            chunk_data = {
                'chunk': chunk_wrapper.get('chunk', {}),
                'chunkRelevanceScore': chunk_wrapper.get('chunkRelevanceScore', 0),
                'source_document': document_name
            }
            
            # Aggiungi informazioni del documento se disponibili
            if document_info:
                chunk_data['document'] = {
                    'name': document_info.get('name'),
                    'displayName': document_info.get('displayName'),
                    'customMetadata': document_info.get('customMetadata', [])
                }
            
            formatted_chunks.append(chunk_data)
        
        return jsonify({
            'success': True,
            'chunks': formatted_chunks,
            'totalCount': len(formatted_chunks),
            'document': document_info,
            'note': 'I chunks sono ordinati per rilevanza. Per vedere tutti i chunks, usa query generiche come "*" o "document".'
        })
        
    except requests.exceptions.RequestException as e:
        logger.error(f"Errore durante recupero chunks: {e}")
        error_detail = {}
        if hasattr(e, 'response') and e.response is not None:
            try:
                error_detail = e.response.json()
            except:
                error_detail = {'error': str(e)}
        logger.error(f"Dettagli errore API: {error_detail}")
        return jsonify({
            'success': False,
            'error': f'Errore nel recupero dei chunks: {str(e)}',
            'details': error_detail
        }), 500
    except Exception as e:
        logger.error(f"Errore imprevisto: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return jsonify({
            'success': False,
            'error': str(e)
        }), 500

@app.errorhandler(413)
def request_entity_too_large(error):
    """Gestisce errori di file troppo grandi"""
    return jsonify({
        'success': False,
        'error': 'File troppo grande. Dimensione massima: 100MB'
    }), 413

@app.errorhandler(500)
def internal_error(error):
    """Gestisce errori interni del server"""
    logger.error(f"Errore interno del server: {str(error)}")
    return jsonify({
        'success': False,
        'error': 'Errore interno del server'
    }), 500

if __name__ == '__main__':
    try:
        validate_config()
        logger.info("Avvio server Flask...")
        # Disabilito use_reloader per evitare problemi con watchdog su Windows
        app.run(debug=True, host='0.0.0.0', port=5000, use_reloader=False)
    except ValueError as e:
        logger.error(f"Errore di configurazione: {str(e)}")
        print(f"\n❌ ERRORE: {str(e)}")
        print("Assicurati di aver configurato correttamente il file .env")

